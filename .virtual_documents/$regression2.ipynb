


from secret import key
import requests
import json
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from collections import defaultdict

from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

import time


FILENAME = 'us-state-abbreviations.txt'
with open(FILENAME, 'r') as file:
    # Read the file contents and split by lines
    state_abbreviations = [line.strip() for line in file.readlines()]


def get_st_df(state, student_size=1000):
    # Base URL for the College Scorecard API
    url = "https://api.data.gov/ed/collegescorecard/v1/schools"
    
    # Your API key (replace with your own key)
    api_key = key
    
    fields = ['school.name',
              # ---- just added this feature! 
              'school.region_id',
              'latest.student.size',
              'school.state',
              'latest.admissions.admission_rate.overall',
              'latest.admissions.sat_scores.average.overall',
              'latest.admissions.act_scores.midpoint.cumulative',
              'latest.admissions.test_requirements',
              f'student.size__range={student_size}..']
    
    params = {
        'api_key': api_key,
        'school.state': state, 
        'fields': ','.join(fields), 
        'page': 0,  # Page number for pagination

        # ---- NEEED TO INCREASE THIS NUMBER!
        'per_page': 100  # Number of records per page (you can adjust this)
    }
    
    # Send the GET request
    response = requests.get(url, params=params)
    state_data = response.json()['results']

    return state_data 


def get_all_states_data(key, st_list):
    """ Iterates through all states and collects the data for each
    Params:
    - key = API key
    Returns:
    A dictionary with states as keys and corresponding school data as values """
    
    # Dictionary to store data for all states
    all_states_data = []
    
    # Loop through all states
    for state in st_list:
        print(f"Retrieving data for {state}...")
        state_data = get_st_df(state)
        
        all_states_data.append(state_data)
        
    # # Combine all state data into a single DataFrame
    # combined_df = pd.concat(all_states_data, ignore_index=True)
    return all_states_data


# gather all the state data!
all_states = get_all_states_data(key, state_abbreviations)


flattened_data = [school for state_data in all_states for school in state_data]

# Convert to a DataFrame
df = pd.DataFrame(flattened_data)

df


# --- analyze NaN value count 
nan_count = df.isna().sum()
nan_count


# ---- need to rename the column names 
df.rename(columns={
    'latest.student.size': 'size',
    'latest.admissions.admission_rate.overall': 'admin_rate',
    'latest.admissions.sat_scores.average.overall': 'avg_sat',
    'latest.admissions.act_scores.midpoint.cumulative': 'midpoint_act',
    'latest.admissions.test_requirements': 'test_requirements',
    'school.name': 'name',
    'school.state': 'state',
    'school.region_id': 'region_id'
}, inplace=True)


# --- convert numerical requirements to strings 
test_labels = {
        0: 'Not Required',
        1: 'Required',
        2: 'Recommended',
        3: 'Niether Rec. or Req.',
        4: 'Not Known'}

df['test_requirements'] = df['test_requirements'].map(
    lambda x: test_labels.get(x, 'Considered but not Req.'))


# ----- do hot-encoding instead of mapping numbers 
df_encoded = pd.get_dummies(df['test_requirements'], prefix='test', dtype=float)
df_encoded


# merge the dataframes together! 
df_merged = pd.concat([df, df_encoded], axis=1)

# don't need the test_requirements column anymore
df_merged.drop("test_requirements", axis=1)

df_merged.head()





def add_bias_column(X):
    """
    Args:
        X (array): can be either 1-d or 2-d
    
    Returns:
        Xnew (array): the same array, but 2-d with a column of 1's in the first spot
    """
    
    # If the array is 1-d
    if len(X.shape) == 1:
        Xnew = np.column_stack([np.ones(X.shape[0]), X])
    
    # If the array is 2-d
    elif len(X.shape) == 2:
        bias_col = np.ones((X.shape[0], 1))
        Xnew = np.hstack([bias_col, X])
        
    else:
        raise ValueError("Input array must be either 1-d or 2-d")

    return Xnew


def line_of_best_fit(X, y):
    """ Args: 
            X (array): either 1d or 2d with the predictor values 
            y (array): 1d and corresponding response values to 'X'
        Returns: 
            m (vector): vector with the slope and intercept term of the line of best fit"""
    # first call the add_bias_column for line of best fit calculation 
    Xnew = add_bias_column(X)
    
    # get the inverse of X transpose 
    XtXinv = np.linalg.inv(np.matmul(Xnew.T, Xnew))
    
    # get the vector with the slope and intercept term
    m = np.matmul(XtXinv, np.matmul(Xnew.T, y))

    return m


def linreg_predict(Xnew, ynew, m):
    """ Args: 
            Xnew (array): 1d or 2d with all predictor values, not including bias term
            ynew (array): 1d with all corresponding response values to 'Xnew'
            m (array): 1d array that contains the coefficents form the line_of_best_fit function
        Returns:
            result_dict (dct): A dct with 4 key-value pairs - ypreds, resids, mse, and r2"""
    
    # add bias column 
    Xnew = add_bias_column(Xnew)

    # fetch all the calculations
    ypreds = np.matmul(Xnew, m)

    resids = ynew - ypreds
    
    mse = np.mean(resids ** 2)

    # use of sklearn built-in r2_score function 
    r2 = r2_score(ynew, ypreds)

    # set up key-value pairs in the resulting dictionary 
    result_dict = {'ypreds': ypreds,
                  'resids': resids,
                  'mse': mse,
                  'r2': r2}

    return result_dict





# Experimenting with dropping rows with more than one NaN value

# # this means each row only has MAX 1 NaN value 
# drop_df = df.dropna(thresh=len(df.columns) - 1)
drop_df = df_merged.dropna()
drop_df.head()


# ------ CHECK THE SHAPE!
drop_df.shape, df.shape


nan_count = drop_df.isna().sum()
nan_count


# # look at the rows with NaN values
# rows_with_nan = drop_df[drop_df.isna().any(axis=1)]
# rows_with_nan


# def get_admin_rate_for_schools(df, state) -> int:
#     state_df = pd.DataFrame()
#     state_df = df[['admin_rate', 'state']].groupby(by='state').mean()
    
#     if state is None:
#         print("State is None.")
#     else:
#         return state_df.loc[state, 'admin_rate']

# def get_avg_sat_for_schools(df, state) -> float:
#     state_df = pd.DataFrame()
#     state_df = df[['avg_sat', 'state']].groupby(by='state').mean()
    
#     if state is None:
#         print("State is None.")
#     else:
#         return state_df.loc[state, 'avg_sat']
    
# def get_midpoint_act_for_schools(df, state) -> float:
#     state_df = pd.DataFrame()
#     state_df = df[['midpoint_act', 'state']].groupby(by='state').mean()
    
#     if state is None:
#         print("State is None.")
#     else:
#         return state_df.loc[state, 'midpoint_act']

# def fill_missing_values(df) -> None:
#     for index, row in df.iterrows():
#         if pd.isna(row['admin_rate']):
#             admin = get_admin_rate_for_schools(df, row['state'])
#             df.at[index, 'admin_rate'] = admin
#         if pd.isna(row['avg_sat']):
#             sat = get_avg_sat_for_schools(df, row['state'])
#             df.at[index, 'avg_sat'] = sat
#         if pd.isna(row['midpoint_act']):
#             act = get_midpoint_act_for_schools(df, row['state'])
#             df.at[index, 'midpoint_act'] = act
#     return df 











# CROSS-FOLD VALIDATION
X = drop_df['avg_sat']
y = drop_df['admin_rate']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# now fit model using line_of_best_fit function
m_train = line_of_best_fit(X_train, y_train)

# use test data to evaulate the model using the linreg_predict function, get "m" param from fitting the model above 
results = linreg_predict(X_test, y_test, m_train)

print(f"MSE: {results['mse']}, R^2: {results['r2']}")


# fit the regression model to the full data set 
m_full = line_of_best_fit(drop_df["avg_sat"], drop_df["admin_rate"])
slope = m_full[1]
intercept = m_full[0]


X = drop_df["avg_sat"]

y_pred = slope * X + intercept


# --- INTIAL PLOT to see fit of the simple regression 
plt.scatter(X, drop_df["admin_rate"], label='data', alpha=0.5)

plt.plot(X, y_pred, color='black',label='linear fit')

plt.xlabel("Average SAT Score")
plt.ylabel("Admission Rate")
plt.title("Simple Regression Plot")
plt.show()








drop_df.columns


nan_count = drop_df.isna().sum()
nan_count


# feat_list = ['size', 'avg_sat', 'midpoint_act','test_Considered but not Req.',
#        'test_Niether Rec. or Req.', 'test_Not Required', 'test_Required']

# X_multi = drop_df[feat_list]
# y = drop_df['admin_rate']

# # scale the continuous features using standardization
# df_scaled = pd.DataFrame()

# continuous_feat = ['size', 'avg_sat', 'midpoint_act']

# for feat in continuous_feat:
#     df_scaled[f'{feat}_scaled'] = ((X_multi[feat] - X_multi[feat].mean()) / X_multi[feat].std())

# df_scaled = pd.concat([df_scaled, drop_df[['test_Considered but not Req.',
#        'test_Niether Rec. or Req.', 'test_Not Required', 'test_Required']]], axis=1)

# # convert dataframe to numpy array 
# X_scaled = np.array(df_scaled)


# need to take a different approach to standarization
from sklearn.preprocessing import StandardScaler

# Instantiate StandardScaler
scaler = StandardScaler()

feat_list = ['size', 'avg_sat', 'midpoint_act','test_Considered but not Req.',
       'test_Niether Rec. or Req.', 'test_Not Required', 'test_Required']

X_multi = drop_df[feat_list]

# Standardize the data
standardized_features = scaler.fit_transform(X_multi)

# Convert back to DataFrame for interpretability
standardized_df = pd.DataFrame(standardized_features, columns=X_multi.columns)

X_scaled = np.array(standardized_df)


X_scaled





# # CROSS VALIDATION
# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# # now fit model using line_of_best_fit function
# m_multi = line_of_best_fit(X_train, y_train)

# # use test data to evaulate the model using the linreg_predict function, get "m" param from fitting the model above 
# results = linreg_predict(X_test, y_test, m_multi)

# print(f"MSE: {results['mse']}, R^2: {results['r2']}")


# ---- REORGANIZE THIS CODE!
def get_ypreds(Xnew, m):
    """add in docstring! """
    # add bias column 
    Xnew = add_bias_column(Xnew)

    # fetch all the calculations
    ypreds = np.matmul(Xnew, m)
    
    return ypreds


# turn this code block into a function 
def loo_cv(X_new, y):
    """ add in docstring here! """
    y_preds = np.empty(len(y))

    # this is extracted from the linreg_predict function 

    # loop through each observation
    for obs in range(len(y)):
        # the below excludes the single row/true y belonging to obs as well as the bias column
        loocv_trainX = np.concatenate((X_new[:obs, 1].reshape(-1,1), X_new[obs+1:, 1].reshape(-1,1)))
        loocv_trainy = np.concatenate((y[:obs], y[obs+1:]))

        m = line_of_best_fit(loocv_trainX, loocv_trainy)

        # Xnew = add_bias_column(X_new) # --- this might be unecessary 
        # ypreds = np.matmul(Xnew, m)

        # y_preds[obs] = linreg_predict(X_new[obs,1].reshape(-1,1), None, m)['ypreds'][0]
        y_preds[obs] = get_ypreds(X_new[obs,1].reshape(-1,1), m)[0]

    # save the scores 
    r_squared = r2_score(y, y_preds)
    mse = ((y - y_preds)**2).mean()
    
    return r_squared, mse 


r_squaredA, mseA = loo_cv(X_scaled, y)
print(f"R^2: {r_squaredA}, MSE: {mseA}")





feat_list2 = ['avg_sat', 'midpoint_act', 'size']

X_multi = drop_df[feat_list2]

# scale the features using standardization
df_scaled = pd.DataFrame()

for feat in X_multi.columns:
    df_scaled[f'{feat}_scaled'] = ((X_multi[feat] - X_multi[feat].mean()) / X_multi[feat].std())

# convert dataframe to numpy array 
X_scaled = np.array(df_scaled)


# PCA implementation 
from sklearn.decomposition import PCA

pca = PCA(n_components = 2)
X_pca = pca.fit_transform(X_scaled)


# CROSS VALIDATION
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

# now fit model using line_of_best_fit function
m_multi_pca = line_of_best_fit(X_train, y_train)

# use test data to evaulate the model using the linreg_predict function, get "m" param from fitting the model above 
results = linreg_predict(X_test, y_test, m_multi_pca)

print(f"MSE: {results['mse']}, R^2: {results['r2']}")





# LOO-CV
r_squaredB, mseB = loo_cv(X_pca, y)
print(f"R^2: {r_squaredB}, MSE: {mseB}")








from sklearn.preprocessing import PolynomialFeatures

# ------- this is a single poly regression test 
X_scores = np.array(drop_df['avg_sat']).reshape(-1,1)


# setting up the cubic model
poly = PolynomialFeatures(3)
X_poly = poly.fit_transform(X_scores)

# drop the last bias column 
X_poly = X_poly[:, 1:]
X_poly


# perform single-fold cross validation 
Xtrain, Xtest, ytrain, ytest = train_test_split(X_poly, drop_df['admin_rate'], test_size=0.30, random_state=42)

# fit to training data 
m_poly = line_of_best_fit(Xtrain, ytrain)

# pass testing data 
poly_rlst = linreg_predict(Xtest, ytest, m_poly)


print(f"MSE: {poly_rlst['mse']}, R^2: {poly_rlst['r2']}")


r_squaredP, mseP = loo_cv(X_poly, y)
print(f"R^2: {r_squaredP}, MSE: {mseP}")


# now let's try multiple poly! 
poly = PolynomialFeatures(3)
Xm_poly = poly.fit_transform(X_scaled)

# drop the last bias column 
Xm_poly = Xm_poly[:, 1:]
Xm_poly


# perform single-fold cross validation 
Xtrain, Xtest, ytrain, ytest = train_test_split(Xm_poly, drop_df['admin_rate'], test_size=0.30, random_state=42)

# fit to training data 
m_poly = line_of_best_fit(Xtrain, ytrain)

# pass testing data 
poly_rlst = linreg_predict(Xtest, ytest, m_poly)


print(f"MSE: {poly_rlst['mse']}, R^2: {poly_rlst['r2']}")


# LOO-CV
r_squaredMP, mseMP = loo_cv(Xm_poly, y)
print(f"R^2: {r_squaredMP}, MSE: {mseMP}")





# pick out features 
drop_df.columns





import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.cluster import KMeans


k_means_feat = ['size', 'admin_rate', 'avg_sat']
X = drop_df[k_means_feat]

# need to standarize the data! 
X_scaled = pd.DataFrame()

for feat in X.columns:
    X_scaled[f'{feat_scaled}'] = ((X[feat] - X[feat].mean()) / X[feat].std())


#Find optimum number of cluster

sse = [] #SUM OF SQUARED ERROR
for k in range(1,11):
    km = KMeans(n_clusters=k, random_state=2)
    km.fit(X)
    sse.append(km.inertia_)


sns.set_style("whitegrid")
g=sns.lineplot(x=range(1,11), y=sse)

g.set(xlabel ="Number of cluster (k)", 
      ylabel = "Sum Squared Error", 
      title ='Elbow Method')

plt.show()


# ELBOW seems to be 3
kmeans = KMeans(n_clusters = 3, random_state = 42)
kmeans.fit(X_scaled)

# find the cluster centers 
kmeans.cluster_centers_

pred = kmeans.fit_predict(X_scaled)
# pred


# PLOT OUT THE CLUSTERS! 
# ------- TURN ALL THIS REPEATING CODE INTO A SINGLE FUNCTION 

# -- PLOT1
plt.figure(figsize=(5,10))
plt.subplot(3, 1, 1)

plt.scatter(X_scaled['size_scaled'],X_scaled['admin_rate_scaled'],c = pred, cmap=cm.Accent)
plt.grid(True)

for center in kmeans.cluster_centers_:
    center = center[:2] # first two dimensions of the centroid
    plt.scatter(center[0],center[1],marker = '^',c = 'red')
# look at isolated features! 
plt.xlabel("size")
plt.ylabel("admin rate")


# -- PLOT 2 
plt.subplot(3, 1, 2)   
plt.scatter(X_scaled['size_scaled'],X_scaled['avg_sat_scaled'],c = pred, cmap=cm.Accent)
plt.grid(True)
for center in kmeans.cluster_centers_:
    # center = center[2:4] # --- what is this doing? 
    center = [center[0], center[2]] 
    plt.scatter(center[0],center[1],marker = '^',c = 'red')
plt.xlabel("size")
plt.ylabel("avg sat")

# -- PLOT 3 
plt.subplot(3, 1, 3)   
plt.scatter(X_scaled['admin_rate_scaled'],X_scaled['avg_sat_scaled'],c = pred, cmap=cm.Accent)
plt.grid(True)
for center in kmeans.cluster_centers_:
    center = center[1:] # --- what is this doing? 
    plt.scatter(center[0],center[1],marker = '^',c = 'red')
plt.xlabel("admin rate")
plt.ylabel("avg sat")


plt.show()


# -- checking the standarization 
X_scaled


from itertools import combinations

def plot_clustering(X):
    """ add in docstring """
    plt.figure(figsize=(5,10))
    
    for combo in list(combinations(X.columns, 2)):
        plt.subplot(len(X.columns), 1, i) # i doesn't exist here!    
        plt.scatter(X_scaled[combo[0]],X_scaled[combo[1]'],c = pred, cmap=cm.Accent)
        plt.grid(True)

       # --- NEED TO FIGURE THIS PART OUT!              
        for center in kmeans.cluster_centers_:
            center = center[1:] # --- what is this doing? 
            plt.scatter(center[0],center[1],marker = '^',c = 'red')
            
        plt.xlabel(f"{combo[0]}")
        plt.ylabel(f"{combo[1]}")

    plt.show()
        


# Running the code
def test_type() -> int, str:
    if sat_or_act.lower() == "sat":
        sat_score = input("SAT Score: ")
        return sat_score, "act"
    elif sat_or_act.lower() == "act":
        act_score = input("ACT Score: ")
        return act_score, "sat"
    else:
        raise ValueError(f"{sat_or_act} is an invalid test.")

def ask_for_state(df) -> DataFrame:
    state = input("What state would you like to look at? (e.g. AZ, MA, ...): ")
    
    if state.upper() in df['school.state'].unique():
        state_df = get_state_df(df, state)
        return state_df
    else:
        raise ValueError(f"{state} is an invalid state.")
        
def get_state_df(df, state) -> DataFrame:
    state_dfs = []

    for school in df['school.state'].unique():
        state_specific_df = df[df['school.state'] == state]
    
        state_dfs.append(state_specific_df)

    state_df = pd.concat(state_dfs, ignore_index=True)
    
    return state_df

def get_reach(state_df, score):
    reach_schools = []
    school = []
    +100
    
    for index in state_df['latest.admissions.sat_scores.average.overall'].unique():
        if df['latest.admissions.sat_scores.average.overall'].unique() >= score+100:
            school.append(df['school.name'].unique())
            school.append(df['latest.admissions.sat_scores.average.overall'].unique())
    
def run():
    sat_or_act = input("What test would you like to use? (ACT or SAT): ")
    score, test = test_type()
    
    print("Fetching all college data...")
    all_states = get_all_states_data(key, state_abbreviations)
    flattened_data = [school for state_data in all_states for school in state_data]

    df = pd.DataFrame(flattened_data)
    
    state_df = ask_for_state(df)
    
    
    print(state_df)
              
    
        
run()
    


# Run this cell to run the project!
run()
